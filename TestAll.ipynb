{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- encoding: utf-8 -*-\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "# import argparse\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from utils import pad, VOCAB, tag2idx, idx2tag,MAX_LEN\n",
    "\n",
    "#加载自定义模型\n",
    "from modules.BBC import Bert_BiGRU_CRF\n",
    "from crf import Bert_BiLSTM_CRF\n",
    "# from modules.BIC import BiGRU_CRF,BiLSTM_CRF\n",
    "# from modules.BC import Bert_CRF\n",
    "# from modules.ABC import ALNET_BiGRU_CRF\n",
    "# from modules.RBC import ROBERTA_BiGRU_CRF\n",
    "# from modules.BBAC import Bert_BiGRU_Attention_CRF\n",
    "\n",
    "from modules.RBLC import ROBERTA_BiLSTM_CRF\n",
    "from modules.ABLC import ALBert_BiLSTM_CRF\n",
    "from modules.EBLC import Ernie_BiLSTM_CRF\n",
    "from modules.XBLC import ALNET_BiLSTM_CRF\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AlbertTokenizer, AlbertModel\n",
    "#from pytorch_pretrained_bert import BertTokenizer\n",
    "from pytorch_transformers import RobertaTokenizer,BertTokenizer\n",
    "from pytorch_transformers import RobertaTokenizer\n",
    "from pytorch_transformers import XLNetModel,XLNetTokenizer\n",
    "\n",
    "# 用来加载日志\n",
    "import datetime\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"use device:\",device)\n",
    "\n",
    "LOGPATH=\"./logs/allmin_v1_\"+datetime.datetime.now().strftime('%Y-%m-%d')+\".log\"\n",
    "LOGPATH\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s %(name)s %(levelname)s %(message)s\",\n",
    "#                     datefmt='%a %d %b %Y %H:%M:%S',\n",
    "                    filename=LOGPATH\n",
    "                    )\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerDataset(Dataset):\n",
    "    def __init__(self, f_path):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('chinese_L-12_H-768_A-12')\n",
    "        with open(f_path, 'r', encoding='utf-8') as fr:\n",
    "            entries = fr.read().strip().split('\\n\\n')\n",
    "        sents, tags_li = [], [] # list of lists\n",
    "        for entry in entries:\n",
    "            words = [line.split()[0] for line in entry.splitlines()]\n",
    "            tags = ([line.split()[-1] for line in entry.splitlines()])\n",
    "            if len(words) > MAX_LEN:\n",
    "                # 先对句号分段\n",
    "                word, tag = [], []\n",
    "                for char, t in zip(words, tags):\n",
    "                    \n",
    "                    if char != '。':\n",
    "                        if char != '\\ue236':   # 测试集中有这个字符\n",
    "                            word.append(char)\n",
    "                            tag.append(t)\n",
    "                    else:\n",
    "                        sents.append([\"[CLS]\"] + word[:MAX_LEN] + [\"[SEP]\"])\n",
    "                        tags_li.append(['[CLS]'] + tag[:MAX_LEN] + ['[SEP]'])\n",
    "                        word, tag = [], []            \n",
    "                # 最后的末尾\n",
    "                if len(word):\n",
    "                    sents.append([\"[CLS]\"] + word[:MAX_LEN] + [\"[SEP]\"])\n",
    "                    tags_li.append(['[CLS]'] + tag[:MAX_LEN] + ['[SEP]'])\n",
    "                    word, tag = [], []\n",
    "            else:\n",
    "                sents.append([\"[CLS]\"] + words[:MAX_LEN] + [\"[SEP]\"])\n",
    "                tags_li.append(['[CLS]'] + tags[:MAX_LEN] + ['[SEP]'])\n",
    "        self.sents, self.tags_li = sents, tags_li\n",
    "                \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words, tags = self.sents[idx], self.tags_li[idx]\n",
    "        x, y = [], []\n",
    "        is_heads = []\n",
    "        for w, t in zip(words, tags):\n",
    "            tokens = self.tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n",
    "            xx = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "            # assert len(tokens) == len(xx), f\"len(tokens)={len(tokens)}, len(xx)={len(xx)}\"\n",
    "\n",
    "            # 中文没有英文wordpiece后分成几块的情况\n",
    "            is_head = [1] + [0]*(len(tokens) - 1)\n",
    "            t = [t] + ['<PAD>'] * (len(tokens) - 1)\n",
    "            yy = [tag2idx[each] for each in t]  # (T,)\n",
    "\n",
    "            x.extend(xx)\n",
    "            is_heads.extend(is_head)\n",
    "            y.extend(yy)\n",
    "        assert len(x)==len(y)==len(is_heads), f\"len(x)={len(x)}, len(y)={len(y)}, len(is_heads)={len(is_heads)}\"\n",
    "\n",
    "        # seqlen\n",
    "        seqlen = len(y)\n",
    "\n",
    "        # to string\n",
    "        words = \" \".join(words)\n",
    "        tags = \" \".join(tags)\n",
    "        return words, x, is_heads, tags, y, seqlen\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERNIEDataset(Dataset):\n",
    "    def __init__(self, f_path):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"./ernie-1.0\")\n",
    "        \n",
    "        with open(f_path, 'r', encoding='utf-8') as fr:\n",
    "            entries = fr.read().strip().split('\\n\\n')\n",
    "        sents, tags_li = [], [] # list of lists\n",
    "        for entry in entries:\n",
    "            words = [line.split()[0] for line in entry.splitlines()]\n",
    "            tags = ([line.split()[-1] for line in entry.splitlines()])\n",
    "            if len(words) > MAX_LEN:\n",
    "                # 先对句号分段\n",
    "                word, tag = [], []\n",
    "                for char, t in zip(words, tags):\n",
    "                    \n",
    "                    if char != '。':\n",
    "                        if char != '\\ue236':   # 测试集中有这个字符\n",
    "                            word.append(char)\n",
    "                            tag.append(t)\n",
    "                    else:\n",
    "                        sents.append([\"[CLS]\"] + word[:MAX_LEN] + [\"[SEP]\"])\n",
    "                        tags_li.append(['[CLS]'] + tag[:MAX_LEN] + ['[SEP]'])\n",
    "                        word, tag = [], []            \n",
    "                # 最后的末尾\n",
    "                if len(word):\n",
    "                    sents.append([\"[CLS]\"] + word[:MAX_LEN] + [\"[SEP]\"])\n",
    "                    tags_li.append(['[CLS]'] + tag[:MAX_LEN] + ['[SEP]'])\n",
    "                    word, tag = [], []\n",
    "            else:\n",
    "                sents.append([\"[CLS]\"] + words[:MAX_LEN] + [\"[SEP]\"])\n",
    "                tags_li.append(['[CLS]'] + tags[:MAX_LEN] + ['[SEP]'])\n",
    "        self.sents, self.tags_li = sents, tags_li\n",
    "                \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        words, tags = self.sents[idx], self.tags_li[idx]\n",
    "        x, y = [], []\n",
    "        is_heads = []\n",
    "        for w, t in zip(words, tags):\n",
    "            tokens = self.tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n",
    "            xx = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "            # assert len(tokens) == len(xx), f\"len(tokens)={len(tokens)}, len(xx)={len(xx)}\"\n",
    "\n",
    "            # 中文没有英文wordpiece后分成几块的情况\n",
    "            is_head = [1] + [0]*(len(tokens) - 1)\n",
    "            t = [t] + ['<PAD>'] * (len(tokens) - 1)\n",
    "            yy = [tag2idx[each] for each in t]  # (T,)\n",
    "\n",
    "            x.extend(xx)\n",
    "            is_heads.extend(is_head)\n",
    "            y.extend(yy)\n",
    "        assert len(x)==len(y)==len(is_heads), f\"len(x)={len(x)}, len(y)={len(y)}, len(is_heads)={len(is_heads)}\"\n",
    "\n",
    "        # seqlen\n",
    "        seqlen = len(y)\n",
    "\n",
    "        # to string\n",
    "        words = \" \".join(words)\n",
    "        tags = \" \".join(tags)\n",
    "        return words, x, is_heads, tags, y, seqlen\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlBERTDataset(Dataset):\n",
    "    def __init__(self, f_path):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"./albert_chinese_base\")\n",
    "        \n",
    "        with open(f_path, 'r', encoding='utf-8') as fr:\n",
    "            entries = fr.read().strip().split('\\n\\n')\n",
    "        sents, tags_li = [], [] # list of lists\n",
    "        for entry in entries:\n",
    "            words = [line.split()[0] for line in entry.splitlines()]\n",
    "            tags = ([line.split()[-1] for line in entry.splitlines()])\n",
    "            if len(words) > MAX_LEN:\n",
    "                # 先对句号分段\n",
    "                word, tag = [], []\n",
    "                for char, t in zip(words, tags):\n",
    "                    \n",
    "                    if char != '。':\n",
    "                        if char != '\\ue236':   # 测试集中有这个字符\n",
    "                            word.append(char)\n",
    "                            tag.append(t)\n",
    "                    else:\n",
    "                        sents.append([\"[CLS]\"] + word[:MAX_LEN] + [\"[SEP]\"])\n",
    "                        tags_li.append(['[CLS]'] + tag[:MAX_LEN] + ['[SEP]'])\n",
    "                        word, tag = [], []            \n",
    "                # 最后的末尾\n",
    "                if len(word):\n",
    "                    sents.append([\"[CLS]\"] + word[:MAX_LEN] + [\"[SEP]\"])\n",
    "                    tags_li.append(['[CLS]'] + tag[:MAX_LEN] + ['[SEP]'])\n",
    "                    word, tag = [], []\n",
    "            else:\n",
    "                sents.append([\"[CLS]\"] + words[:MAX_LEN] + [\"[SEP]\"])\n",
    "                tags_li.append(['[CLS]'] + tags[:MAX_LEN] + ['[SEP]'])\n",
    "        self.sents, self.tags_li = sents, tags_li\n",
    "                \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        words, tags = self.sents[idx], self.tags_li[idx]\n",
    "        x, y = [], []\n",
    "        is_heads = []\n",
    "        for w, t in zip(words, tags):\n",
    "            tokens = self.tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n",
    "            xx = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "            # assert len(tokens) == len(xx), f\"len(tokens)={len(tokens)}, len(xx)={len(xx)}\"\n",
    "\n",
    "            # 中文没有英文wordpiece后分成几块的情况\n",
    "            is_head = [1] + [0]*(len(tokens) - 1)\n",
    "            t = [t] + ['<PAD>'] * (len(tokens) - 1)\n",
    "            yy = [tag2idx[each] for each in t]  # (T,)\n",
    "\n",
    "            x.extend(xx)\n",
    "            is_heads.extend(is_head)\n",
    "            y.extend(yy)\n",
    "        assert len(x)==len(y)==len(is_heads), f\"len(x)={len(x)}, len(y)={len(y)}, len(is_heads)={len(is_heads)}\"\n",
    "\n",
    "        # seqlen\n",
    "        seqlen = len(y)\n",
    "\n",
    "        # to string\n",
    "        words = \" \".join(words)\n",
    "        tags = \" \".join(tags)\n",
    "        return words, x, is_heads, tags, y, seqlen\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XlNetDataset(Dataset):\n",
    "    def __init__(self, f_path):\n",
    "        self.tokenizer = XLNetTokenizer.from_pretrained('chinese_xlnet_base_pytorch')\n",
    "        with open(f_path, 'r', encoding='utf-8') as fr:\n",
    "            entries = fr.read().strip().split('\\n\\n')\n",
    "        sents, tags_li = [], [] # list of lists\n",
    "        for entry in entries:\n",
    "            words = [line.split()[0] for line in entry.splitlines()]\n",
    "            tags = ([line.split()[-1] for line in entry.splitlines()])\n",
    "            if len(words) > MAX_LEN:\n",
    "                # 先对句号分段\n",
    "                word, tag = [], []\n",
    "                for char, t in zip(words, tags):\n",
    "                    \n",
    "                    if char != '。':\n",
    "                        if char != '\\ue236':   # 测试集中有这个字符\n",
    "                            word.append(char)\n",
    "                            tag.append(t)\n",
    "                    else:\n",
    "                        sents.append([\"[CLS]\"] + word[:MAX_LEN] + [\"[SEP]\"])\n",
    "                        tags_li.append(['[CLS]'] + tag[:MAX_LEN] + ['[SEP]'])\n",
    "                        word, tag = [], []            \n",
    "                # 最后的末尾\n",
    "                if len(word):\n",
    "                    sents.append([\"[CLS]\"] + word[:MAX_LEN] + [\"[SEP]\"])\n",
    "                    tags_li.append(['[CLS]'] + tag[:MAX_LEN] + ['[SEP]'])\n",
    "                    word, tag = [], []\n",
    "            else:\n",
    "                sents.append([\"[CLS]\"] + words[:MAX_LEN] + [\"[SEP]\"])\n",
    "                tags_li.append(['[CLS]'] + tags[:MAX_LEN] + ['[SEP]'])\n",
    "        self.sents, self.tags_li = sents, tags_li\n",
    "                \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words, tags = self.sents[idx], self.tags_li[idx]\n",
    "        x, y = [], []\n",
    "        is_heads = []\n",
    "        for w, t in zip(words, tags):\n",
    "            tokens = self.tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n",
    "            xx = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "            # assert len(tokens) == len(xx), f\"len(tokens)={len(tokens)}, len(xx)={len(xx)}\"\n",
    "\n",
    "            # 中文没有英文wordpiece后分成几块的情况\n",
    "            is_head = [1] + [0]*(len(tokens) - 1)\n",
    "            t = [t] + ['<PAD>'] * (len(tokens) - 1)\n",
    "            yy = [tag2idx[each] for each in t]  # (T,)\n",
    "\n",
    "            x.extend(xx)\n",
    "            is_heads.extend(is_head)\n",
    "            y.extend(yy)\n",
    "        assert len(x)==len(y)==len(is_heads), f\"len(x)={len(x)}, len(y)={len(y)}, len(is_heads)={len(is_heads)}\"\n",
    "\n",
    "        # seqlen\n",
    "        seqlen = len(y)\n",
    "\n",
    "        # to string\n",
    "        words = \" \".join(words)\n",
    "        tags = \" \".join(tags)\n",
    "        return words, x, is_heads, tags, y, seqlen\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(iterator):\n",
    "        words, x, is_heads, tags, y, seqlens = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        _y = y # for monitoring\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.neg_log_likelihood(x, y) # logits: (N, T, VOCAB), y: (N, T)\n",
    "\n",
    "        # logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n",
    "        # y = y.view(-1)  # (N*T,)\n",
    "        # writer.add_scalar('data/loss', loss.item(), )\n",
    "\n",
    "        # loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # if i==0:\n",
    "        #     print(\"=====sanity check======\")\n",
    "        #     #print(\"words:\", words[0])\n",
    "        #     print(\"x:\", x.cpu().numpy()[0][:seqlens[0]])\n",
    "        #     # print(\"tokens:\", tokenizer.convert_ids_to_tokens(x.cpu().numpy()[0])[:seqlens[0]])\n",
    "        #     print(\"is_heads:\", is_heads[0])\n",
    "        #     print(\"y:\", _y.cpu().numpy()[0][:seqlens[0]])\n",
    "        #     print(\"tags:\", tags[0])\n",
    "        #     print(\"seqlen:\", seqlens[0])\n",
    "        #     print(\"=======================\")\n",
    "\n",
    "        if i%100==0: # monitoring\n",
    "            print(f\"step: {i}, loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from codecs import open\n",
    "from collections import Counter\n",
    "\n",
    "def count_correct_tags(gold_lists,pre_lists):\n",
    "    \"\"\"计算每种标签预测正确的个数(对应精确率、召回率计算公式上的tp)，用于后面精确率以及召回率的计算\"\"\"\n",
    "    correct_dict = {}\n",
    "\n",
    "    for gold_tag, predict_tag in zip(gold_lists, pre_lists):\n",
    "        if gold_tag == predict_tag:\n",
    "            if gold_tag not in correct_dict:\n",
    "                correct_dict[gold_tag] = 1\n",
    "            else:\n",
    "                correct_dict[gold_tag] += 1\n",
    "\n",
    "    return correct_dict\n",
    "\n",
    "\n",
    "\n",
    "def cal_precision(pre_tags_counter,correct_tags_number,tagset):\n",
    "    precision_scores = {}\n",
    "    for tag in tagset:\n",
    "        if pre_tags_counter[tag] == 0:\n",
    "            precision_scores[tag] = 0\n",
    "        else:\n",
    "            precision_scores[tag] = correct_tags_number.get(tag, 0) / \\\n",
    "                                    pre_tags_counter[tag]\n",
    "\n",
    "    return precision_scores\n",
    "\n",
    "\n",
    "def cal_recall(golden_tags_counter,correct_tags_number,tagset):\n",
    "    recall_scores = {}\n",
    "    for tag in tagset:\n",
    "        recall_scores[tag] = correct_tags_number.get(tag, 0) / \\\n",
    "                             golden_tags_counter[tag]\n",
    "    return recall_scores\n",
    "\n",
    "\n",
    "def cal_f1(precision_scores,recall_scores,tagset):\n",
    "    f1_scores = {}\n",
    "    for tag in tagset:\n",
    "        p, r = precision_scores[tag], recall_scores[tag]\n",
    "        f1_scores[tag] = 2 * p * r / (p + r + 1e-10)  # 加上一个特别小的数，防止分母为0\n",
    "    return f1_scores\n",
    "\n",
    "\n",
    "def report_scores(precision_scores,recall_scores,f1_scores,golden_tags_counter,gold_lists,tagset):\n",
    "    \"\"\"将结果用表格的形式打印出来，像这个样子：\n",
    "\n",
    "                  precision    recall  f1-score   support\n",
    "          B-LOC      0.775     0.757     0.766      1084\n",
    "          I-LOC      0.601     0.631     0.616       325\n",
    "         B-MISC      0.698     0.499     0.582       339\n",
    "         I-MISC      0.644     0.567     0.603       557\n",
    "          B-ORG      0.795     0.801     0.798      1400\n",
    "          I-ORG      0.831     0.773     0.801      1104\n",
    "          B-PER      0.812     0.876     0.843       735\n",
    "          I-PER      0.873     0.931     0.901       634\n",
    "\n",
    "      avg/total      0.779     0.764     0.770      6178\n",
    "    \"\"\"\n",
    "    # 打印表头\n",
    "    header_format = '{:>9s}  {:>9} {:>9} {:>9} {:>9}'\n",
    "    header = ['precision', 'recall', 'f1-score', 'support']\n",
    "    print(header_format.format('', *header))\n",
    "\n",
    "    row_format = '{:>9s}  {:>9.4f} {:>9.4f} {:>9.4f} {:>9}'\n",
    "    # 打印每个标签的 精确率、召回率、f1分数\n",
    "    for tag in tagset:\n",
    "        print(row_format.format(\n",
    "            tag,\n",
    "            precision_scores[tag],\n",
    "            recall_scores[tag],\n",
    "            f1_scores[tag],\n",
    "            golden_tags_counter[tag]\n",
    "        ))\n",
    "\n",
    "    # 计算并打印平均值\n",
    "    avg_metrics = cal_weighted_average(precision_scores,recall_scores,f1_scores,golden_tags_counter,gold_lists,tagset)\n",
    "    print(row_format.format(\n",
    "        'avg/total',\n",
    "        avg_metrics['precision'],\n",
    "        avg_metrics['recall'],\n",
    "        avg_metrics['f1_score'],\n",
    "        len(gold_lists)\n",
    "    ))\n",
    "\n",
    "\n",
    "def cal_weighted_average(precision_scores,recall_scores,f1_scores,golden_tags_counter,gold_lists,tagset):\n",
    "    weighted_average = {}\n",
    "    total = len(gold_lists)\n",
    "\n",
    "    # 计算weighted precisions:\n",
    "    weighted_average['precision'] = 0.\n",
    "    weighted_average['recall'] = 0.\n",
    "    weighted_average['f1_score'] = 0.\n",
    "    for tag in tagset:\n",
    "        size = golden_tags_counter[tag]\n",
    "        weighted_average['precision'] += precision_scores[tag] * size\n",
    "        weighted_average['recall'] += recall_scores[tag] * size\n",
    "        weighted_average['f1_score'] += f1_scores[tag] * size\n",
    "\n",
    "    for metric in weighted_average.keys():\n",
    "        weighted_average[metric] /= total\n",
    "\n",
    "    return weighted_average\n",
    "\n",
    "\n",
    "def remove_Otags(gold_lists,pre_lists):\n",
    "\n",
    "    length = len(gold_lists)\n",
    "\n",
    "    O_tag_indices = [i for i in range(length)\n",
    "                     if gold_lists[i] == 'O']\n",
    "\n",
    "    gold_lists = [tag for i, tag in enumerate(gold_lists)\n",
    "                        if i not in O_tag_indices]\n",
    "\n",
    "    pre_lists = [tag for i, tag in enumerate(pre_lists)\n",
    "                         if i not in O_tag_indices]\n",
    "    print(\"原总标记数为{}，移除了{}个O标记，占比{:.2f}%\".format(\n",
    "        length,\n",
    "        len(O_tag_indices),\n",
    "        len(O_tag_indices) / length * 100\n",
    "    ))\n",
    "\n",
    "    return gold_lists,pre_lists\n",
    "\n",
    "def get_entities(seq):\n",
    "    \"\"\"\n",
    "    Gets entities from sequence.\n",
    "\n",
    "    Args:\n",
    "        seq (list): sequence of labels.\n",
    "\n",
    "    Returns:\n",
    "        list: list of (chunk_type, chunk_start, chunk_end).\n",
    "\n",
    "    Example:\n",
    "        seq = ['B-PER', 'I-PER', 'O', 'B-LOC']\n",
    "        get_entities(seq)\n",
    "        [('PER', 0, 1), ('LOC', 3, 3)]\n",
    "    \"\"\"\n",
    "    # for nested list\n",
    "    if any(isinstance(s, list) for s in seq):\n",
    "        seq = [item for sublist in seq for item in sublist + ['O']]\n",
    "    prev_tag = 'O'\n",
    "    prev_type = ''\n",
    "    begin_offset = 0\n",
    "    chunks = []\n",
    "    for i, chunk in enumerate(seq + ['O']):\n",
    "        tag = chunk[0]\n",
    "        type_ = chunk.split('-')[-1]\n",
    "\n",
    "        if end_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "            chunks.append((prev_type, begin_offset, i - 1))\n",
    "        if start_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "            begin_offset = i\n",
    "        prev_tag = tag\n",
    "        prev_type = type_\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def end_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "    \"\"\"Checks if a chunk ended between the previous and current word.\n",
    "\n",
    "    Args:\n",
    "        prev_tag: previous chunk tag.\n",
    "        tag: current chunk tag.\n",
    "        prev_type: previous type.\n",
    "        type_: current type.\n",
    "\n",
    "    Returns:\n",
    "        chunk_end: boolean.\n",
    "    \"\"\"\n",
    "    chunk_end = False\n",
    "\n",
    "    if prev_tag == 'S':\n",
    "        chunk_end = True\n",
    "    if prev_tag == 'E':\n",
    "        chunk_end = True\n",
    "    # pred_label中可能出现这种情形\n",
    "    if prev_tag == 'B' and tag == 'B':\n",
    "        chunk_end = True\n",
    "    if prev_tag == 'B' and tag == 'S':\n",
    "        chunk_end = True\n",
    "    if prev_tag == 'B' and tag == 'O':\n",
    "        chunk_end = True\n",
    "    if prev_tag == 'M' and tag == 'B':\n",
    "        chunk_end = True\n",
    "    if prev_tag == 'M' and tag == 'S':\n",
    "        chunk_end = True\n",
    "    if prev_tag == 'M' and tag == 'O':\n",
    "        chunk_end = True\n",
    "\n",
    "    if prev_tag != 'O' and prev_tag != '.' and prev_type != type_:\n",
    "        chunk_end = True\n",
    "\n",
    "    return chunk_end\n",
    "\n",
    "\n",
    "def start_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "    \"\"\"Checks if a chunk started between the previous and current word.\n",
    "\n",
    "    Args:\n",
    "        prev_tag: previous chunk tag.\n",
    "        tag: current chunk tag.\n",
    "        prev_type: previous type.\n",
    "        type_: current type.\n",
    "\n",
    "    Returns:\n",
    "        chunk_start: boolean.\n",
    "    \"\"\"\n",
    "    chunk_start = False\n",
    "\n",
    "    if tag == 'B':\n",
    "        chunk_start = True\n",
    "    if tag == 'S':\n",
    "        chunk_start = True\n",
    "\n",
    "    if prev_tag == 'S' and tag == 'M':\n",
    "        chunk_start = True\n",
    "    if prev_tag == 'O' and tag == 'M':\n",
    "        chunk_start = True\n",
    "    if prev_tag == 'S' and tag == 'E':\n",
    "        chunk_start = True\n",
    "    if prev_tag == 'O' and tag == 'E':\n",
    "        chunk_start = True\n",
    "\n",
    "    if tag != 'O' and tag != '.' and prev_type != type_:\n",
    "        chunk_start = True\n",
    "\n",
    "    return chunk_start\n",
    "\n",
    "\n",
    "def f1_score(y_true, y_pred, labels, mode='dev'):\n",
    "\n",
    "    true_entities = set(get_entities(y_true))\n",
    "    pred_entities = set(get_entities(y_pred))\n",
    "    nb_correct = len(true_entities & pred_entities)\n",
    "    nb_pred = len(pred_entities)\n",
    "    nb_true = len(true_entities)\n",
    "\n",
    "    true_entities_num = {'NAME': 0, 'TYPE': 0, 'ENV': 0, 'FEA': 0, 'SUR': 0, 'EVA': 0, 'DEF': 0}\n",
    "    #true_entities_num = {'CONT': 0, 'TITLE': 0, 'EDU': 0, 'ORG': 0, 'LOC': 0, 'PRO': 0, 'RACE': 0,'NAME':0}\n",
    "    for entity in true_entities:\n",
    "        tag = entity[0]\n",
    "        true_entities_num[tag] += 1\n",
    "\n",
    "    p = nb_correct / nb_pred if nb_pred > 0 else 0\n",
    "    r = nb_correct / nb_true if nb_true > 0 else 0\n",
    "    score = 2 * p * r / (p + r) if p + r > 0 else 0\n",
    "    if mode == 'dev':\n",
    "        return score\n",
    "    else:\n",
    "        f_score = {}\n",
    "        precision_scores = {}\n",
    "        recall_scores = {}\n",
    "        for label in labels:\n",
    "            true_entities_label = set()\n",
    "            pred_entities_label = set()\n",
    "            for t in true_entities:\n",
    "                if t[0] == label:\n",
    "                    true_entities_label.add(t)\n",
    "            for p in pred_entities:\n",
    "                if p[0] == label:\n",
    "                    pred_entities_label.add(p)\n",
    "            nb_correct_label = len(true_entities_label & pred_entities_label)\n",
    "            nb_pred_label = len(pred_entities_label)\n",
    "            nb_true_label = len(true_entities_label)\n",
    "\n",
    "            p_label = nb_correct_label / nb_pred_label if nb_pred_label > 0 else 0\n",
    "            r_label = nb_correct_label / nb_true_label if nb_true_label > 0 else 0\n",
    "            score_label = 2 * p_label * r_label / (p_label + r_label) if p_label + r_label > 0 else 0\n",
    "            f_score[label] = score_label\n",
    "            precision_scores[label] = p_label\n",
    "            recall_scores[label] = r_label\n",
    "\n",
    "        weighted_average = {}\n",
    "        total = nb_true\n",
    "\n",
    "        # 计算weighted precisions:\n",
    "        weighted_average['precision'] = 0.\n",
    "        weighted_average['recall'] = 0.\n",
    "        weighted_average['f1_score'] = 0.\n",
    "        for tag in labels:\n",
    "            size = true_entities_num[tag]\n",
    "            weighted_average['precision'] += precision_scores[tag] * size\n",
    "            weighted_average['recall'] += recall_scores[tag] * size\n",
    "            weighted_average['f1_score'] += f_score[tag] * size\n",
    "\n",
    "        for metric in weighted_average.keys():\n",
    "            weighted_average[metric] /= total\n",
    "\n",
    "        header_format = '{:>9s}  {:>9} {:>9} {:>9} {:>9}'\n",
    "        header = ['precision', 'recall', 'f1-score', 'support']\n",
    "        print(\"-------------------Entities Report------------------\")\n",
    "        print(header_format.format('', *header))\n",
    "\n",
    "        row_format = '{:>9s}  {:>9.4f} {:>9.4f} {:>9.4f} {:>9}'\n",
    "        # 打印每个标签的 精确率、召回率、f1分数\n",
    "        for tag in labels:\n",
    "            print(row_format.format(\n",
    "                tag,\n",
    "                precision_scores[tag],\n",
    "                recall_scores[tag],\n",
    "                f_score[tag],\n",
    "                true_entities_num[tag]\n",
    "            ))\n",
    "        print(row_format.format(\n",
    "            'avg/total',\n",
    "            weighted_average['precision'],\n",
    "            weighted_average['recall'],\n",
    "            weighted_average['f1_score'],\n",
    "            total\n",
    "        ))\n",
    "        print(\"-------------------Labels Report----- -------------\")\n",
    "\n",
    "        return weighted_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, iterator, f, device,temp:str, isEnd=False):\n",
    "#     print(\"temp:\"+temp)\n",
    "    model.eval()\n",
    "\n",
    "    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            words, x, is_heads, tags, y, seqlens = batch\n",
    "            x = x.to(device)\n",
    "            # y = y.to(device)\n",
    "\n",
    "            _, y_hat = model(x)  # y_hat: (N, T)\n",
    "\n",
    "            Words.extend(words)\n",
    "            Is_heads.extend(is_heads)\n",
    "            Tags.extend(tags)\n",
    "            Y.extend(y.numpy().tolist())\n",
    "            Y_hat.extend(y_hat.cpu().numpy().tolist())\n",
    "\n",
    "    ## gets results and save\n",
    "    with open(temp, 'w', encoding='utf-8') as fout:\n",
    "        for words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):\n",
    "            y_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]\n",
    "            preds = [idx2tag[hat] for hat in y_hat]\n",
    "            assert len(preds)==len(words.split())==len(tags.split())\n",
    "            for w, t, p in zip(words.split()[1:-1], tags.split()[1:-1], preds[1:-1]):\n",
    "                fout.write(f\"{w} {t} {p}\\n\")\n",
    "            fout.write(\"\\n\")\n",
    "\n",
    "    word_lists = []\n",
    "    gold_lists = []\n",
    "    pre_lists = []\n",
    "    \n",
    "    with open(temp, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line != '\\n':\n",
    "                word, gold, pre = line.strip('\\n').split()\n",
    "                #print(word, ' ' + gold, '' + pre)\n",
    "                word_lists.append(word)\n",
    "                gold_lists.append(gold)\n",
    "                pre_lists.append(pre)\n",
    "    \n",
    "    \n",
    "    \n",
    "    labels = ['NAME', 'TYPE', 'ENV', 'FEA', 'SUR', 'EVA', 'DEF']\n",
    "    avg=f1_score(gold_lists, pre_lists, labels, mode='test')\n",
    "#     gold_lists,pre_lists=remove_Otags(gold_lists,pre_lists)\n",
    "#     tagset = set(gold_lists)\n",
    "#     correct_tags_number = count_correct_tags(gold_lists,pre_lists)\n",
    "#     pre_tags_counter = Counter(pre_lists)\n",
    "#     golden_tags_counter = Counter(gold_lists)\n",
    "#     precision_scores = cal_precision(pre_tags_counter,correct_tags_number,tagset)\n",
    "#     recall_scores = cal_recall(golden_tags_counter,correct_tags_number,tagset)\n",
    "#     f1_scores = cal_f1(precision_scores,recall_scores,tagset)\n",
    "#     avg = cal_weighted_average(precision_scores,recall_scores,f1_scores,golden_tags_counter,gold_lists,tagset)\n",
    "#     report_scores(precision_scores,recall_scores,f1_scores,golden_tags_counter,gold_lists,tagset)\n",
    "    \n",
    "\n",
    "    if isEnd!=True:\n",
    "        os.remove(temp)\n",
    "\n",
    "#     print(\"precision=%.4f\"%precision)\n",
    "#     print(\"recall=%.4f\"%recall)\n",
    "#     print(\"f1=%.4f\"%f1)\n",
    "    return avg['precision'],avg['recall'], avg['f1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(LOGDIR=\"checkpoints/01\",TRAINSET=\"data/gqp.train\",VALIDSET=\"data/gqp.dev\",TESTSET=\"data/test.txt.anns\",BATCH_SIZE=5,LR=0.001,N_EPOCHS=30):\n",
    "\n",
    "    \n",
    "    PRFS=[]\n",
    "    modelnames = [\n",
    "#         \"bert_bigru_attention_crf\",\n",
    "                  \"ernie_bilstm_crf\",\n",
    "                \"bert_bilstm_crf\",\n",
    "#                \"bilstm_crf\",\n",
    "#                \"bigru_crf\",\n",
    "#                \"bert_crf\",\n",
    "                \"bert_bigru_crf\"\n",
    "#                 \"alnet_bilstm_crf\"\n",
    "            ]\n",
    "    models =  [ \n",
    "#                 (Bert_BiGRU_Attention_CRF(tag2idx).to(device),\"bert_bigru_attention_crf_temp\"),\n",
    "#                (Ernie_BiLSTM_CRF(tag2idx).to(device),\"ernie_bilstm_crf_temp\")\n",
    "                (Ernie_BiLSTM_CRF(tag2idx).to(device),\"ernie_bilstm_crf_temp\"),\n",
    "                (Bert_BiLSTM_CRF(tag2idx).to(device),\"bert_bilstm_crf_temp\")\n",
    "#                ,(BiLSTM_CRF(tag2idx).to(device),\"bilstm_crf_temp\")\n",
    "#                ,(BiGRU_CRF(tag2idx).to(device),\"bigru_crf_temp\")\n",
    "#                ,(Bert_CRF(tag2idx).to(device),\"bert_crf_temp\")\n",
    "                ,(Bert_BiGRU_CRF(tag2idx).to(device),\"bert_bigru_crf_temp\")     \n",
    "#                 (ALNET_BiLSTM_CRF(tag2idx).to(device),\"alnet_bilstm_crf_temp\")\n",
    "                ]\n",
    "        \n",
    "    for index,(currentmodel,currentmodeltemp) in enumerate(models):\n",
    "        train_iter,eval_iter = None,None\n",
    "        \n",
    "        # 根据模型名称加载对应的模型数据\n",
    "        if modelnames[index] in [\"bert_bilstm_crf\",\"bilstm_crf\",\"bert_bigru_crf\"]:\n",
    "            train_dataset = NerDataset(TRAINSET)\n",
    "            eval_dataset = NerDataset(VALIDSET)\n",
    "            test_dataset = NerDataset(TESTSET)\n",
    "            train_iter = data.DataLoader(dataset=train_dataset,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 shuffle=True,\n",
    "                                 num_workers=8,\n",
    "                                 collate_fn=pad)\n",
    "            eval_iter = data.DataLoader(dataset=eval_dataset,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 shuffle=True,\n",
    "                                 num_workers=8,\n",
    "                                 collate_fn=pad)\n",
    "            test_iter = data.DataLoader(dataset=test_dataset,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 shuffle=True,\n",
    "                                 num_workers=8,\n",
    "                                 collate_fn=pad)\n",
    "            logger.info(f\"{modelnames[index]}匹配到NerDataset\")\n",
    "            \n",
    "        elif modelnames[index] in [\"roberta_bilstm_crf\"]:\n",
    "            \n",
    "            train_dataset = RoBERTaDataset(TRAINSET)\n",
    "            eval_dataset = RoBERTaDataset(VALIDSET)\n",
    "            test_dataset = RoBERTaDataset(TESTSET)\n",
    "            train_iter = data.DataLoader(dataset=train_dataset,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 shuffle=True,\n",
    "                                 num_workers=3,\n",
    "                                 collate_fn=pad)\n",
    "            eval_iter = data.DataLoader(dataset=eval_dataset,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 shuffle=True,\n",
    "                                 num_workers=3,\n",
    "                                 collate_fn=pad)\n",
    "            test_iter = data.DataLoader(dataset=test_dataset,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 shuffle=True,\n",
    "                                 num_workers=3,\n",
    "                                 collate_fn=pad)\n",
    "            logger.info(f\"{modelnames[index]}匹配到RoBERTaDataset\")\n",
    "            \n",
    "        elif modelnames[index] in [\"alnet_bilstm_crf\"]:\n",
    "            train_dataset = XlNetDataset(TRAINSET)\n",
    "            eval_dataset = XlNetDataset(VALIDSET)\n",
    "            test_dataset = XlNetDataset(TESTSET)\n",
    "            train_iter = data.DataLoader(dataset=train_dataset,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 shuffle=True,\n",
    "                                 num_workers=8,\n",
    "                                 collate_fn=pad)\n",
    "            eval_iter = data.DataLoader(dataset=eval_dataset,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 shuffle=True,\n",
    "                                 num_workers=8,\n",
    "                                 collate_fn=pad)\n",
    "            test_iter = data.DataLoader(dataset=test_dataset,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 shuffle=True,\n",
    "                                 num_workers=8,\n",
    "                                 collate_fn=pad)\n",
    "            logger.info(f\"{modelnames[index]}匹配到XlNetDataset\")\n",
    "            \n",
    "        elif modelnames[index] in [\"albert_bilstm_crf\"]:\n",
    "            \n",
    "            train_dataset = AlBERTDataset(TRAINSET)\n",
    "            eval_dataset = AlBERTDataset(VALIDSET)\n",
    "            test_dataset = AlBERTDataset(TESTSET)\n",
    "            train_iter = data.DataLoader(dataset=train_dataset,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 shuffle=True,\n",
    "                                 num_workers=3,\n",
    "                                 collate_fn=pad)\n",
    "            eval_iter = data.DataLoader(dataset=eval_dataset,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 shuffle=True,\n",
    "                                 num_workers=3,\n",
    "                                 collate_fn=pad)\n",
    "            test_iter = data.DataLoader(dataset=test_dataset,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 shuffle=True,\n",
    "                                 num_workers=3,\n",
    "                                 collate_fn=pad)\n",
    "            logger.info(f\"{modelnames[index]}匹配到AlBERTDataset\")\n",
    "            \n",
    "        elif modelnames[index] in [\"ernie_bilstm_crf\"]:\n",
    "            \n",
    "            train_dataset = ERNIEDataset(TRAINSET)\n",
    "            eval_dataset = ERNIEDataset(VALIDSET)\n",
    "            test_dataset = ERNIEDataset(TESTSET)\n",
    "            train_iter = data.DataLoader(dataset=train_dataset,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 shuffle=True,\n",
    "                                 num_workers=8,\n",
    "                                 collate_fn=pad)\n",
    "            eval_iter = data.DataLoader(dataset=eval_dataset,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 shuffle=True,\n",
    "                                 num_workers=8,\n",
    "                                 collate_fn=pad)\n",
    "            test_iter = data.DataLoader(dataset=test_dataset,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 shuffle=True,\n",
    "                                 num_workers=8,\n",
    "                                 collate_fn=pad)\n",
    "            logger.info(f\"{modelnames[index]}匹配到ERNIEDataset\")\n",
    "        else:\n",
    "            logger.warning(f\"{modelnames[index]}没有匹配到模型的数据\")\n",
    "        \n",
    "        \n",
    "        \n",
    "#         print(\"=\"*30+\"begin \"+ currentmodelname + \"=\"*30)\n",
    "        optimizer = optim.Adam(currentmodel.parameters(), lr = LR)\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=0) \n",
    "        start = time.time()\n",
    "        \n",
    "        for epoch in range(1, N_EPOCHS+1):  # 每个epoch对dev集进行测试\n",
    "            \n",
    "            print(\"=========epoch= {} ===model= {} =========\".format(epoch,modelnames[index]))\n",
    "            train(currentmodel, train_iter, optimizer, criterion, device)\n",
    "            print(\"-----------save model= {} ---------------\".format(modelnames[index]))\n",
    "            torch.save(currentmodel.state_dict(), \"./savemodels/\"+modelnames[index]+\".pkl\")\n",
    "            #save_model(currentmodel, \"./savemodels/\"+modelnames[index]+\".pkl\")\n",
    "            if not os.path.exists(LOGDIR): os.makedirs(LOGDIR)\n",
    "            fname = os.path.join(LOGDIR, str(epoch))\n",
    "#             precision, recall, f1 = eval(currentmodel, eval_iter, fname, device,currentmodeltemp,isEnd=False)\n",
    "#             logger.info(\"modelname=%s,epoch=%03d,precision=%.4f,recall=%.4f,f1=%.4f\"%(modelnames[index],epoch,precision, recall, f1))\n",
    "            if epoch == N_EPOCHS:\n",
    "#                 print(\"========= Test ===model= {} ============\".format(modelnames[index]))\n",
    "                print(\"训练完毕,共用时{}秒.\".format(int(time.time()-start)))\n",
    "                precision, recall, f1 = eval(currentmodel, eval_iter, fname, device,currentmodeltemp,isEnd=True)\n",
    "                logger.info(\"modelname=%s,epoch=%03d,precision=%.4f,recall=%.4f,f1=%.4f\"%(modelnames[index],epoch,precision, recall, f1))\n",
    "                PRFS.append([precision, recall, f1])\n",
    "            \n",
    "#         print(\"=\"*30+\"end \"+ currentmodelname + \"=\"*30)\n",
    "    return PRFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelNames = [\"albert_bilstm_crf\",\"ernie_bilstm_crf\",\"roberta_bilstm_crf\",\"alnet_bilstm_crf\"]\n",
    "modelNames = [\"alnet_bilstm_crf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************** BATCH SIZE = 20 **************************\n",
      "=========epoch= 1 ===model= ernie_bilstm_crf =========\n",
      "step: 0, loss: 10021.814453125\n",
      "step: 100, loss: 9538.064453125\n",
      "step: 200, loss: 9521.541015625\n",
      "step: 300, loss: 9521.5126953125\n",
      "step: 400, loss: 9519.2724609375\n",
      "-----------save model= ernie_bilstm_crf ---------------\n",
      "训练完毕,共用时86秒.\n",
      "-------------------Entities Report------------------\n",
      "           precision    recall  f1-score   support\n",
      "     NAME     0.0000    0.0000    0.0000        61\n",
      "     TYPE     0.0000    0.0000    0.0000       135\n",
      "      ENV     0.2971    0.1502    0.1995      1691\n",
      "      FEA     0.2540    0.1331    0.1747      1427\n",
      "      SUR     0.0000    0.0000    0.0000        59\n",
      "      EVA     0.0180    0.0062    0.0092       484\n",
      "      DEF     0.0000    0.0000    0.0000       174\n",
      "avg/total     0.2167    0.1109    0.1467      4031\n",
      "-------------------Labels Report----- -------------\n",
      "=========epoch= 1 ===model= bert_bilstm_crf =========\n",
      "step: 0, loss: 9880.115234375\n",
      "step: 100, loss: 9539.2578125\n",
      "step: 200, loss: 9527.384765625\n",
      "step: 300, loss: 9509.4638671875\n",
      "step: 400, loss: 9516.134765625\n",
      "-----------save model= bert_bilstm_crf ---------------\n",
      "训练完毕,共用时91秒.\n",
      "-------------------Entities Report------------------\n",
      "           precision    recall  f1-score   support\n",
      "     NAME     0.0000    0.0000    0.0000        61\n",
      "     TYPE     0.0000    0.0000    0.0000       135\n",
      "      ENV     0.0250    0.0148    0.0186      1691\n",
      "      FEA     0.4124    0.1682    0.2389      1427\n",
      "      SUR     0.0000    0.0000    0.0000        59\n",
      "      EVA     0.0256    0.0021    0.0038       484\n",
      "      DEF     0.0000    0.0000    0.0000       174\n",
      "avg/total     0.1595    0.0660    0.0928      4031\n",
      "-------------------Labels Report----- -------------\n",
      "=========epoch= 1 ===model= bert_bigru_crf =========\n",
      "step: 0, loss: 9882.12890625\n",
      "step: 100, loss: 9542.2880859375\n",
      "step: 200, loss: 9530.24609375\n",
      "step: 300, loss: 9512.2783203125\n",
      "step: 400, loss: 9499.642578125\n",
      "-----------save model= bert_bigru_crf ---------------\n",
      "训练完毕,共用时87秒.\n",
      "-------------------Entities Report------------------\n",
      "           precision    recall  f1-score   support\n",
      "     NAME     0.0000    0.0000    0.0000        61\n",
      "     TYPE     0.0000    0.0000    0.0000       135\n",
      "      ENV     0.2516    0.2957    0.2719      1691\n",
      "      FEA     0.5184    0.4744    0.4954      1427\n",
      "      SUR     0.0000    0.0000    0.0000        59\n",
      "      EVA     0.0043    0.0021    0.0028       484\n",
      "      DEF     0.0000    0.0000    0.0000       174\n",
      "avg/total     0.2896    0.2922    0.2898      4031\n",
      "-------------------Labels Report----- -------------\n"
     ]
    }
   ],
   "source": [
    "iters=[]\n",
    "# range(10,210,10)\n",
    "# current=None\n",
    "#iterLength=[*range(1,2,1)]\n",
    "# iterLength=[*range(1,12,2)]\n",
    "# for thisIter in iterLength:\n",
    "# #     print(thisIter)\n",
    "    \n",
    "#     current=run(LOGDIR=\"checkpoints/01\",TRAINSET=\"data/train.txt.anns\",VALIDSET=\"data/dev.txt.anns\",BATCH_SIZE=20,LR=5e-5,N_EPOCHS=thisIter)\n",
    "#     iters.append(current)\n",
    "# # print(iters)\n",
    "# iters = np.array(iters)\n",
    "# # iters\n",
    "batches=[20]\n",
    "for batch in batches:\n",
    "    print(\"************** BATCH SIZE = {} **************************\".format(batch))\n",
    "    current=run(LOGDIR=\"checkpoints/01\",TRAINSET=\"data/train.txt.anns\",VALIDSET=\"data/dev.txt.anns\",TESTSET=\"data/test.txt.anns\",\n",
    "            BATCH_SIZE=batch,LR=5e-5,N_EPOCHS=1)\n",
    "# current=run(LOGDIR=\"checkpoints/01\",TRAINSET=\"data/train.txt.anns\",VALIDSET=\"data/dev.txt.anns\",TESTSET=\"data/test.txt.anns\",\n",
    "#             BATCH_SIZE=65,LR=5e-5,N_EPOCHS=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
